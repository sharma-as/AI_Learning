{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 5 â€” LoRA and QLoRA Finetuning\n",
        "\n",
        "This notebook demonstrates parameter-efficient finetuning for causal language models using LoRA and, when available, QLoRA (4-bit) with bitsandbytes.\n",
        "\n",
        "- Minimal dependencies: uses `transformers`, `peft`, optional `bitsandbytes`.\n",
        "- Small in-memory toy dataset to avoid external downloads.\n",
        "- Works on CPU (LoRA) and CUDA GPUs (LoRA/QLoRA).\n",
        "- Toggle `DO_QLORA` to enable 4-bit quantization if GPU + bitsandbytes are available.\n",
        "\n",
        "Tip: Start with the tiny model preset (default). Switch to a larger model once the flow runs end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment check\n",
        "import sys, platform\n",
        "import importlib\n",
        "\n",
        "def _try_import(name):\n",
        "    try:\n",
        "        mod = importlib.import_module(name)\n",
        "        return mod, True\n",
        "    except Exception as e:\n",
        "        print(f'Optional import failed: {name} -> {type(e).__name__}: {e}')\n",
        "        return None, False\n",
        "\n",
        "import torch\n",
        "print(f'Python : {sys.version.split()[0]} on {platform.system()}')\n",
        "print(f'Torch  : {torch.__version__}')\n",
        "print('CUDA   :', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU    :', torch.cuda.get_device_name(0))\n",
        "\n",
        "transformers, _ = _try_import('transformers')\n",
        "peft, _ = _try_import('peft')\n",
        "bnb, has_bnb = _try_import('bitsandbytes')\n",
        "print('transformers:', getattr(transformers, '__version__', 'not found'))\n",
        "print('peft        :', getattr(peft, '__version__', 'not found'))\n",
        "print('bitsandbytes:', getattr(bnb, '__version__', 'not found'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install (if needed)\n",
        "If imports above failed, install packages and restart the kernel.\n",
        "\n",
        "- Core: `transformers`, `peft`, `accelerate`\n",
        "- QLoRA: `bitsandbytes` (Linux CUDA recommended)\n",
        "\n",
        "Note: Commented by default to avoid accidental installs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U transformers peft accelerate\n",
        "# For QLoRA on Linux/CUDA:\n",
        "# !pip install -U bitsandbytes\n",
        "# After installing, restart the kernel and rerun from the top."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "Set model, toggles, and training hyperparameters.\n",
        "\n",
        "- Default model is tiny to ensure quick runs.\n",
        "- Set `DO_QLORA = True` to attempt 4-bit loading if GPU + bitsandbytes are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Choose a tiny model for speed. You can switch to 'distilgpt2' later.\n",
        "MODEL_NAME = 'sshleifer/tiny-gpt2'  # or 'distilgpt2'\n",
        "\n",
        "# Toggle QLoRA. Requires GPU + bitsandbytes.\n",
        "DO_QLORA = False\n",
        "\n",
        "# Training hyperparameters (kept small to run quickly)\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM_STEPS = 2\n",
        "LR = 2e-4\n",
        "MAX_SEQ_LEN = 256\n",
        "WARMUP_STEPS = 10\n",
        "WEIGHT_DECAY = 0.0\n",
        "LOGGING_STEPS = 10\n",
        "OUTPUT_DIR = 'outputs-day5-lora'\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print({'device': device, 'model': MODEL_NAME, 'q_lora': DO_QLORA})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tiny Toy Dataset\n",
        "We create a small instruction/response dataset in-memory to avoid external downloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        'instruction': 'Write a short greeting to a new team member named Alex.',\n",
        "        'response': 'Welcome to the team, Alex! We\'re excited to work with you.'\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Explain what LoRA is in one sentence.',\n",
        "        'response': 'LoRA is a parameter-efficient finetuning method that adds small trainable adapters to a frozen base model.'\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Give one pro and one con of QLoRA.',\n",
        "        'response': 'Pro: drastically reduces memory usage; Con: may slightly impact model quality compared to full precision.'\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Summarize: \"LoRA helps reduce the number of trainable parameters.\"',\n",
        "        'response': 'LoRA reduces the trainable parameters via low-rank adapters.'\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Provide a one-line tip for training small models.',\n",
        "        'response': 'Use smaller sequence lengths and batch sizes to fit memory.'\n",
        "    },\n",
        "]\n",
        "\n",
        "def format_example(ex: Dict[str, str]) -> str:\n",
        "    return (\n",
        "        f\"### Instruction:\\n{ex['instruction']}\\n\\n### Response:\\n{ex['response']}\"\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "texts = [format_example(e) for e in examples]\n",
        "split = int(0.8 * len(texts))\n",
        "train_texts = texts[:split]\n",
        "val_texts = texts[split:]\n",
        "\n",
        "def encode_texts(texts: List[str]):\n",
        "    input_ids, attention_mask = [], []\n",
        "    for t in texts:\n",
        "        enc = tokenizer(\n",
        "            t,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LEN,\n",
        "            padding=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # Append EOS if space\n",
        "        if enc['input_ids'].shape[1] < MAX_SEQ_LEN and tokenizer.eos_token_id is not None:\n",
        "            eos = torch.tensor([[tokenizer.eos_token_id]])\n",
        "            enc['input_ids'] = torch.cat([enc['input_ids'], eos], dim=1)\n",
        "            enc['attention_mask'] = torch.cat([enc['attention_mask'], torch.ones_like(eos)], dim=1)\n",
        "        input_ids.append(enc['input_ids'][0])\n",
        "        attention_mask.append(enc['attention_mask'][0])\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "train_ids, train_mask = encode_texts(train_texts)\n",
        "val_ids, val_mask = encode_texts(val_texts)\n",
        "\n",
        "class SimpleLMDataset:\n",
        "    def __init__(self, ids_list, mask_list):\n",
        "        self.ids_list = ids_list\n",
        "        self.mask_list = mask_list\n",
        "    def __len__(self):\n",
        "        return len(self.ids_list)\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.ids_list[idx]\n",
        "        m = self.mask_list[idx]\n",
        "        return {\n",
        "            'input_ids': x,\n",
        "            'attention_mask': m,\n",
        "            'labels': x.clone(),\n",
        "        }\n",
        "\n",
        "train_ds = SimpleLMDataset(train_ids, train_mask)\n",
        "val_ds = SimpleLMDataset(val_ids, val_mask)\n",
        "\n",
        "# Collator that pads and sets -100 on label padding tokens\n",
        "def collate_fn(features: List[Dict[str, Any]]):\n",
        "    batch = {}\n",
        "    # Pad input_ids and attention_mask\n",
        "    ids = [f['input_ids'] for f in features]\n",
        "    attn = [f['attention_mask'] for f in features]\n",
        "    ids = torch.nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attn = torch.nn.utils.rnn.pad_sequence(attn, batch_first=True, padding_value=0)\n",
        "    labels = ids.clone()\n",
        "    labels[ids == tokenizer.pad_token_id] = -100\n",
        "    batch['input_ids'] = ids\n",
        "    batch['attention_mask'] = attn\n",
        "    batch['labels'] = labels\n",
        "    return batch\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Base Model and Apply LoRA/QLoRA\n",
        "- If `DO_QLORA=True` and your environment supports bitsandbytes + CUDA, loads 4-bit.\n",
        "- Otherwise, uses standard LoRA with full/fp16 weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "bnb_config = None\n",
        "use_qlora = False\n",
        "\n",
        "if DO_QLORA and torch.cuda.is_available():\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type='nf4',\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16,\n",
        "        )\n",
        "        use_qlora = True\n",
        "    except Exception as e:\n",
        "        print('Falling back to LoRA (no 4-bit):', e)\n",
        "\n",
        "print('Loading base model...')\n",
        "model_kwargs = {}\n",
        "if use_qlora:\n",
        "    model_kwargs.update(dict(device_map='auto', quantization_config=bnb_config))\n",
        "else:\n",
        "    if torch.cuda.is_available():\n",
        "        model_kwargs.update(dict(torch_dtype=torch.float16))\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
        "model.config.use_cache = False  # Important for gradient checkpointing\n",
        "\n",
        "if use_qlora:\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception as e:\n",
        "        print('Could not enable gradient checkpointing:', e)\n",
        "\n",
        "# Setup LoRA\n",
        "target_modules = ['c_attn', 'c_proj']  # works for GPT-2 family\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_cfg)\n",
        "print('Trainable parameters in PEFT model:')\n",
        "def print_trainable_parameters(model):\n",
        "    trainable = 0\n",
        "    total = 0\n",
        "    for _, p in model.named_parameters():\n",
        "        total += p.numel()\n",
        "        if p.requires_grad:\n",
        "            trainable += p.numel()\n",
        "    print(f'Trainable: {trainable:,} / Total: {total:,} ({100 * trainable/total:.2f}%)')\n",
        "print_trainable_parameters(peft_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train\n",
        "Uses the standard `Trainer` for causal LM with our simple collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "fp16 = torch.cuda.is_available() and not use_qlora  # QLoRA uses 4-bit + bf16/fp16 internally\n",
        "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 if torch.cuda.is_available() else False\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    fp16=bool(fp16),\n",
        "    bf16=bool(bf16 and not fp16),\n",
        "    report_to=[],  # disable W&B etc.\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds if len(val_ds) > 0 else None,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "train_result = trainer.train()\n",
        "print('Training complete.')\n",
        "try:\n",
        "    metrics = trainer.evaluate()\n",
        "    print(metrics)\n",
        "except Exception as e:\n",
        "    print('Evaluation skipped:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Inference\n",
        "Generate a few responses to sanity-check the finetuned adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "gen = pipeline(\n",
        "    'text-generation',\n",
        "    model=peft_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "test_prompts = [\n",
        "    '### Instruction:\\nSay hi to Sam\\n\\n### Response:\\n',\n",
        "    '### Instruction:\\nWhat is LoRA?\\n\\n### Response:\\n',\n",
        "]\n",
        "\n",
        "for p in test_prompts:\n",
        "    out = gen(p, max_new_tokens=40, do_sample=True, top_p=0.9, temperature=0.7)[0]['generated_text']\n",
        "    print('-' * 80)\n",
        "    print(textwrap.fill(out, 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Adapters and (Optionally) Merge\n",
        "- Save LoRA/QLoRA adapters with `save_pretrained`.\n",
        "- Optionally merge with the base model and save a full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "adapter_dir = Path(OUTPUT_DIR) / 'lora_adapter'\n",
        "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "peft_model.save_pretrained(str(adapter_dir))\n",
        "print('Saved LoRA adapters to:', adapter_dir)\n",
        "\n",
        "# Optional: merge and save a standalone model (useful for inference without PEFT)\n",
        "try:\n",
        "    merged = peft_model.merge_and_unload()\n",
        "    full_dir = Path(OUTPUT_DIR) / 'merged_model'\n",
        "    full_dir.mkdir(parents=True, exist_ok=True)\n",
        "    merged.save_pretrained(str(full_dir))\n",
        "    tokenizer.save_pretrained(str(full_dir))\n",
        "    print('Saved merged model to:', full_dir)\n",
        "except Exception as e:\n",
        "    print('Merging skipped:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- For real tasks, replace the toy dataset with your own or a Hugging Face dataset.\n",
        "- Increase `MAX_SEQ_LEN`, `NUM_EPOCHS`, and choose a larger base model.\n",
        "- For QLoRA, ensure: Linux, CUDA GPU, and a working `bitsandbytes` install.\n",
        "- If you switch models (e.g., to LLaMA/OPT), adjust `target_modules` accordingly."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
